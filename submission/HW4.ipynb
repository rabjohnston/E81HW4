{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Neural Network Deep Dive\n",
    "\n",
    "Rob Johnston and Haris Memic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Deep Neural Network\n",
    "\n",
    "3a) Train and test a generic deep neural network on the data. Record the training time (your choice of approaches) and the accuracy on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Convolutional Network\n",
    "\n",
    "3b) Train and test a generic convolutional network on the data. Record the training time (your choice of approaches) and the accuracy on the test set.\n",
    " 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Non-Neural Network\n",
    "\n",
    "3c) Train and test a non-NN algorithm with reasonable performance on the data. I would suggest not SVM for speed reasons. I would suggest avoiding Naïve Bayes and Decision Tree as they generally lack accuracy, but might be worth a try as a supplement. As before, record the training time (your choice of approaches) and the accuracy on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of models\n",
    "\n",
    "3d) Comment on the differences and perhaps plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity to training data\n",
    "\n",
    "4) Assess the sensitivity of the test set to training data size [10]\n",
    "4a) Randomly divide the training data into at least 3 pieces (like cross-validation): A, B, C, ... but do so in a way that preserves the an equal fraction of the 10 categories of images across sets. That is, each set {A,B, etc.} should have the same number of category 1, 2,...10 images within it.\n",
    "4b) Train your classifiers in part 3 on each of the following, A, AB, etc. and test on the full test set 4c) Plot and comment on how quickly the classifiers learn as compared to 3) using the full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Distortion\n",
    "\n",
    "5) Assess the sensitivity of the test set to distortions [15 pts]\n",
    "5a) Create at least one additional test sets by distorting the test set in one manner (not the training\n",
    "data). Possible choices include:\n",
    " Adding random noise to the images of different levels\n",
    " Shifting the brightness or contrast by different levels\n",
    " Flipping or transposing the images\n",
    " Randomly settting different fractions of the pixels to an intermediate value\n",
    " Obscuring the top k rows of the images to an intermediate value (different levels of k)\n",
    "These can be done on the fly in testing or separately. TensorFlow includes some options in their tf.images package but similar results could be obtained using GIMP, for example.\n",
    "5b) Repeat the baseline run analysis in 3) on the distorted images 5c) Comment on what you learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning of hyper-parameters\n",
    "\n",
    "6) Compare the tuning of hyperparameters across architectural models [25 points]\n",
    "6a) Select at least 2 architectural models where an architectural model can include:\n",
    " arrangement of layers in a CNN\n",
    " number of hidden layers\n",
    " number of nodes in hidden layers\n",
    " order of pooling/convolution/etc. in CNN, and/or\n",
    " CNN vs. deep NN\n",
    "2\n",
    "October 15 2pm\n",
    "6b) Compare the architectures in terms of speed & accuracy across  2 hyperparameters such as:  gradient descent optimization methods such as momentum, ADAM, Adagrad, etc.\n",
    " normalization effects\n",
    " regularization\n",
    " dropout\n",
    "6c) Plot the results and comment on what you learned. Compare the errors (or accuracy) as a\n",
    "function of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximisng performance of CNN\n",
    "\n",
    "7) Maximize your performance with your deep neural network or the CNN [30 pts]\n",
    "a. Tune the parameters\n",
    "b. Discover what works best and walk us through what you tried\n",
    "c. Submit the performance as a %accuracy and as a 10x10 table in the format showing the\n",
    "counts of the test images actual vs. predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Points\n",
    "\n",
    "8) Exploratory points [optional]\n",
    "Exploratory points will be given if you go beyond these methods.\n",
    "ideas for this part that could include different data sets, different types of networks, a significantly deeper analysis into the tuning perhaps, etc. Include a separate section highlighting what you have done that is worthy of exploratory points so we don’t overlook your effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
